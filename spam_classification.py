# -*- coding: utf-8 -*-
"""spam_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lKTIkhIFkYgPwnxLr71BsZEwfvgiEMvl
"""

import pandas as pd
spam_df = pd.read_csv("C:/Users/My-Asus/Downloads/spam.csv")
spam_df.head()

spam_df = spam_df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])

spam_df.head()

category = pd.get_dummies(spam_df.v1)
df_baru = pd.concat([spam_df, category], axis=1)
df_baru = df_baru.drop(columns='v1')
df_baru

df_baru.isnull().sum()

df_baru.shape

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

ps = PorterStemmer()

corpus = []

for i in range(len(df_baru)):
    review = re.sub('[^a-zA-Z]',' ',df_baru['v2'][i])
    review = review.lower()
    review = review.split()

    review = [ps.stem(word) for word in review if word not in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

corpus

email = df_baru['v2'].values
label = df_baru[['ham', 'spam']].values

from sklearn.model_selection import train_test_split
email_train, email_test, label_train, label_test = train_test_split(email, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=1000, oov_token='x')
tokenizer.fit_on_texts(email_train)
tokenizer.fit_on_texts(email_test)

sequence_train = tokenizer.texts_to_sequences(email_train)
sequence_test = tokenizer.texts_to_sequences(email_test)

padded_train = pad_sequences(sequence_train)
padded_test = pad_sequences(sequence_test)

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.98):
      print("\nAkurasi telah mencapai >98%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 10
history = model.fit(padded_train, label_train, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()